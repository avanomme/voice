#!/usr/bin/env python3
"""
Voice Assistant Web UI with Multiple Voices and Thinking Display
Provides a web interface for the voice assistant using FastAPI
"""

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, UploadFile, File
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, FileResponse
import whisper
import subprocess
import tempfile
import requests
import json
import os
import asyncio
import base64
import io
import wave
from pathlib import Path
import urllib.request

app = FastAPI(title="Voice Assistant", description="Local Voice Assistant with Ollama")

# Voice models configuration
VOICE_MODELS = {
    "lessac_medium": {
        "name": "Lessac (Medium) - Clear American",
        "url": "https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/lessac/medium/en_US-lessac-medium.onnx",
        "config_url": "https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/lessac/medium/en_US-lessac-medium.onnx.json",
        "filename": "en_US-lessac-medium.onnx"
    },
    "amy_medium": {
        "name": "Amy (Medium) - Friendly Female",
        "url": "https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/amy/medium/en_US-amy-medium.onnx",
        "config_url": "https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/amy/medium/en_US-amy-medium.onnx.json",
        "filename": "en_US-amy-medium.onnx"
    },
    "danny_low": {
        "name": "Danny (Low) - Male Casual",
        "url": "https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/danny/low/en_US-danny-low.onnx",
        "config_url": "https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/danny/low/en_US-danny-low.onnx.json",
        "filename": "en_US-danny-low.onnx"
    },
    "kathleen_low": {
        "name": "Kathleen (Low) - Soft Female",
        "url": "https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/kathleen/low/en_US-kathleen-low.onnx",
        "config_url": "https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/kathleen/low/en_US-kathleen-low.onnx.json",
        "filename": "en_US-kathleen-low.onnx"
    },
    "ryan_high": {
        "name": "Ryan (High) - Deep Male",
        "url": "https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/ryan/high/en_US-ryan-high.onnx",
        "config_url": "https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/ryan/high/en_US-ryan-high.onnx.json",
        "filename": "en_US-ryan-high.onnx"
    },
    "joe_medium": {
        "name": "Joe (Medium) - News Anchor",
        "url": "https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/joe/medium/en_US-joe-medium.onnx",
        "config_url": "https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/joe/medium/en_US-joe-medium.onnx.json",
        "filename": "en_US-joe-medium.onnx"
    },
    "kristin_medium": {
        "name": "Kristin (Medium) - Professional Female",
        "url": "https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/kristin/medium/en_US-kristin-medium.onnx",
        "config_url": "https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/kristin/medium/en_US-kristin-medium.onnx.json",
        "filename": "en_US-kristin-medium.onnx"
    },
    "arctic_medium": {
        "name": "Arctic (Medium) - Neutral",
        "url": "https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/arctic/medium/en_US-arctic-medium.onnx",
        "config_url": "https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/arctic/medium/en_US-arctic-medium.onnx.json",
        "filename": "en_US-arctic-medium.onnx"
    }
}

class VoiceAssistant:
    def __init__(self):
        print("Loading Whisper model...")
        self.whisper_model = whisper.load_model("base")
        self.ollama_url = "http://localhost:11434/api/generate"
        self.voices_dir = os.path.expanduser("~/.local/share/piper/voices")
        self.current_voice = "lessac_medium"
        print("Voice Assistant initialized")
        
        # Download all voice models
        self.download_voices()
        
    def download_voices(self):
        """Download all voice models if they don't exist"""
        os.makedirs(self.voices_dir, exist_ok=True)
        
        for voice_id, voice_info in VOICE_MODELS.items():
            model_path = os.path.join(self.voices_dir, voice_info["filename"])
            config_path = model_path + ".json"
            
            if not os.path.exists(model_path):
                print(f"Downloading {voice_info['name']}...")
                try:
                    urllib.request.urlretrieve(voice_info["url"], model_path)
                    urllib.request.urlretrieve(voice_info["config_url"], config_path)
                    print(f"Downloaded {voice_info['name']}")
                except Exception as e:
                    print(f"Failed to download {voice_info['name']}: {e}")
        
    def transcribe(self, audio_data):
        """Convert audio bytes to text"""
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as f:
            f.write(audio_data)
            temp_path = f.name
            
        try:
            result = self.whisper_model.transcribe(temp_path)
            return result["text"].strip()
        finally:
            os.unlink(temp_path)
    
    def query_ollama_with_thinking(self, text):
        """Send text to Ollama and get response with thinking"""
        try:
            # First get the thinking response
            thinking_payload = {
                "model": "qwen3:latest", 
                "prompt": f"Think step by step about this request: {text}\n\nFirst show your thinking process, then provide your final response. Format your response as:\n\nTHINKING:\n[your detailed thinking process]\n\nRESPONSE:\n[your final answer]",
                "stream": False
            }
            response = requests.post(self.ollama_url, json=thinking_payload, timeout=30)
            full_response = response.json()["response"]
            
            # Parse thinking and response
            if "THINKING:" in full_response and "RESPONSE:" in full_response:
                parts = full_response.split("RESPONSE:")
                thinking = parts[0].replace("THINKING:", "").strip()
                response_text = parts[1].strip()
            else:
                # Fallback if format not followed
                thinking = "No structured thinking provided"
                response_text = full_response
                
            return thinking, response_text
        except Exception as e:
            return f"Error: {str(e)}", f"Error connecting to Ollama: {str(e)}"
    
    def speak(self, text, voice_id="lessac_medium"):
        """Convert text to speech using specified Piper voice"""
        try:
            if voice_id not in VOICE_MODELS:
                voice_id = "lessac_medium"
                
            model_path = os.path.join(self.voices_dir, VOICE_MODELS[voice_id]["filename"])
            
            if not os.path.exists(model_path):
                print(f"Voice model {voice_id} not found, using default")
                model_path = os.path.join(self.voices_dir, VOICE_MODELS["lessac_medium"]["filename"])
            
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as f:
                cmd = ["piper", "--model", model_path, "--output_file", f.name]
                process = subprocess.Popen(cmd, stdin=subprocess.PIPE, text=True, 
                                         stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                stdout, stderr = process.communicate(input=text)
                
                if process.returncode != 0:
                    print(f"Piper error: {stderr}")
                    return None
                
                # Read the audio file
                with open(f.name, 'rb') as audio_file:
                    audio_data = audio_file.read()
                    
                os.unlink(f.name)
                return base64.b64encode(audio_data).decode()
        except Exception as e:
            print(f"TTS Error: {e}")
            return None

# Global assistant instance
assistant = VoiceAssistant()

@app.get("/", response_class=HTMLResponse)
async def get_index():
    """Serve the main web interface"""
    voices_options = ""
    for voice_id, voice_info in VOICE_MODELS.items():
        selected = "selected" if voice_id == "lessac_medium" else ""
        voices_options += f'<option value="{voice_id}" {selected}>{voice_info["name"]}</option>'
    
    return HTMLResponse(content=f"""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Assistant</title>
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
            background: #1a1a1a;
            color: #e0e0e0;
        }}
        .container {{
            background: #2d2d2d;
            border-radius: 12px;
            padding: 30px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.3);
        }}
        h1 {{
            text-align: center;
            color: #4CAF50;
            margin-bottom: 30px;
        }}
        .controls {{
            text-align: center;
            margin: 30px 0;
        }}
        button {{
            background: #4CAF50;
            color: white;
            border: none;
            padding: 15px 30px;
            border-radius: 8px;
            font-size: 16px;
            cursor: pointer;
            margin: 0 10px;
            transition: background 0.3s;
        }}
        button:hover:not(:disabled) {{
            background: #45a049;
        }}
        button:disabled {{
            background: #666;
            cursor: not-allowed;
        }}
        .recording {{
            background: #f44336 !important;
        }}
        .chat-container {{
            height: 500px;
            overflow-y: auto;
            border: 1px solid #444;
            border-radius: 8px;
            padding: 15px;
            margin: 20px 0;
            background: #1e1e1e;
        }}
        .message {{
            margin: 15px 0;
            padding: 10px;
            border-radius: 8px;
        }}
        .user-message {{
            background: #0084ff;
            color: white;
            margin-left: 50px;
        }}
        .ai-message {{
            background: #333;
            color: #e0e0e0;
            margin-right: 50px;
        }}
        .thinking-section {{
            background: #2a2a2a;
            border: 1px solid #555;
            border-radius: 8px;
            margin: 10px 0;
        }}
        .thinking-header {{
            background: #3a3a3a;
            padding: 10px 15px;
            border-radius: 8px 8px 0 0;
            cursor: pointer;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }}
        .thinking-header:hover {{
            background: #4a4a4a;
        }}
        .thinking-content {{
            padding: 15px;
            display: none;
            border-top: 1px solid #555;
            font-family: 'Courier New', monospace;
            font-size: 14px;
            line-height: 1.4;
            white-space: pre-wrap;
        }}
        .thinking-content.expanded {{
            display: block;
        }}
        .chevron {{
            transition: transform 0.3s;
        }}
        .chevron.expanded {{
            transform: rotate(90deg);
        }}
        .status {{
            text-align: center;
            padding: 10px;
            border-radius: 4px;
            margin: 10px 0;
        }}
        .status.info {{ background: #2196F3; color: white; }}
        .status.error {{ background: #f44336; color: white; }}
        .status.success {{ background: #4CAF50; color: white; }}
        .audio-controls {{
            margin: 10px 0;
        }}
        audio {{
            width: 100%;
            background: #333;
        }}
        .input-container {{
            display: flex;
            gap: 10px;
            margin: 20px 0;
        }}
        input[type="text"] {{
            flex: 1;
            padding: 12px;
            border: 1px solid #444;
            border-radius: 8px;
            background: #333;
            color: #e0e0e0;
            font-size: 16px;
        }}
        input[type="text"]:focus {{
            outline: none;
            border-color: #4CAF50;
        }}
        .voice-selector {{
            margin: 20px 0;
            text-align: center;
        }}
        select {{
            background: #333;
            color: #e0e0e0;
            border: 1px solid #444;
            border-radius: 8px;
            padding: 10px;
            font-size: 14px;
        }}
        .voice-test {{
            margin-left: 10px;
            padding: 8px 16px;
            font-size: 14px;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>🎤 Voice Assistant with Multiple Voices</h1>
        
        <div class="voice-selector">
            <label for="voiceSelect">Voice: </label>
            <select id="voiceSelect">
                {voices_options}
            </select>
            <button class="voice-test" onclick="testVoice()">Test Voice</button>
        </div>
        
        <div class="input-container">
            <input type="text" id="textInput" placeholder="Type your message here..." />
            <button onclick="sendText()">Send</button>
        </div>
        
        <div class="controls">
            <button id="recordBtn" onclick="toggleRecording()">🎤 Start Recording</button>
            <button onclick="clearChat()">🗑️ Clear Chat</button>
        </div>
        
        <div id="status"></div>
        <div id="chatContainer" class="chat-container"></div>
    </div>

    <script>
        let isRecording = false;
        let mediaRecorder = null;
        let audioChunks = [];
        
        function showStatus(message, type = 'info') {{
            const status = document.getElementById('status');
            status.innerHTML = `<div class="status ${{type}}">${{message}}</div>`;
            setTimeout(() => status.innerHTML = '', 5000);
        }}
        
        function addMessage(content, isUser = false) {{
            const chatContainer = document.getElementById('chatContainer');
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${{isUser ? 'user-message' : 'ai-message'}}`;
            messageDiv.innerHTML = content;
            chatContainer.appendChild(messageDiv);
            chatContainer.scrollTop = chatContainer.scrollHeight;
        }}
        
        function addThinkingSection(thinking, response) {{
            const chatContainer = document.getElementById('chatContainer');
            
            // Create thinking section
            const thinkingDiv = document.createElement('div');
            thinkingDiv.className = 'thinking-section';
            thinkingDiv.innerHTML = `
                <div class="thinking-header" onclick="toggleThinking(this)">
                    <span>🧠 AI Thinking Process</span>
                    <span class="chevron">▶</span>
                </div>
                <div class="thinking-content">${{thinking}}</div>
            `;
            
            chatContainer.appendChild(thinkingDiv);
            
            // Add response
            addMessage(`🤖 ${{response}}`);
            
            chatContainer.scrollTop = chatContainer.scrollHeight;
        }}
        
        function toggleThinking(header) {{
            const content = header.nextElementSibling;
            const chevron = header.querySelector('.chevron');
            
            content.classList.toggle('expanded');
            chevron.classList.toggle('expanded');
        }}
        
        async function toggleRecording() {{
            const recordBtn = document.getElementById('recordBtn');
            
            if (!isRecording) {{
                try {{
                    const stream = await navigator.mediaDevices.getUserMedia({{ audio: true }});
                    mediaRecorder = new MediaRecorder(stream);
                    audioChunks = [];
                    
                    mediaRecorder.ondataavailable = event => {{
                        audioChunks.push(event.data);
                    }};
                    
                    mediaRecorder.onstop = async () => {{
                        const audioBlob = new Blob(audioChunks, {{ type: 'audio/wav' }});
                        await processAudio(audioBlob);
                        stream.getTracks().forEach(track => track.stop());
                    }};
                    
                    mediaRecorder.start();
                    isRecording = true;
                    recordBtn.innerHTML = '⏹️ Stop Recording';
                    recordBtn.className = 'recording';
                    showStatus('Recording... Speak now!');
                    
                }} catch (error) {{
                    showStatus('Microphone access denied or not available', 'error');
                }}
            }} else {{
                mediaRecorder.stop();
                isRecording = false;
                recordBtn.innerHTML = '🎤 Start Recording';
                recordBtn.className = '';
                showStatus('Processing audio...');
            }}
        }}
        
        async function processAudio(audioBlob) {{
            try {{
                const formData = new FormData();
                formData.append('audio', audioBlob, 'recording.wav');
                
                showStatus('Transcribing speech...');
                const response = await fetch('/transcribe', {{
                    method: 'POST',
                    body: formData
                }});
                
                const result = await response.json();
                
                if (result.error) {{
                    showStatus(result.error, 'error');
                    return;
                }}
                
                const transcription = result.transcription;
                if (transcription) {{
                    addMessage(`🗣️ ${{transcription}}`, true);
                    await getAIResponse(transcription);
                }} else {{
                    showStatus('No speech detected', 'error');
                }}
                
            }} catch (error) {{
                showStatus('Error processing audio: ' + error.message, 'error');
            }}
        }}
        
        async function sendText() {{
            const textInput = document.getElementById('textInput');
            const text = textInput.value.trim();
            
            if (!text) return;
            
            addMessage(`💬 ${{text}}`, true);
            textInput.value = '';
            await getAIResponse(text);
        }}
        
        async function getAIResponse(text) {{
            try {{
                showStatus('Getting AI response...');
                
                const voiceSelect = document.getElementById('voiceSelect');
                const selectedVoice = voiceSelect.value;
                
                const response = await fetch('/chat', {{
                    method: 'POST',
                    headers: {{
                        'Content-Type': 'application/json',
                    }},
                    body: JSON.stringify({{ 
                        message: text,
                        voice: selectedVoice
                    }})
                }});
                
                const result = await response.json();
                
                if (result.error) {{
                    showStatus(result.error, 'error');
                    return;
                }}
                
                // Add thinking section and response
                addThinkingSection(result.thinking, result.response);
                
                // Generate speech
                if (result.audio) {{
                    const audioDiv = document.createElement('div');
                    audioDiv.className = 'audio-controls';
                    audioDiv.innerHTML = `<audio controls autoplay><source src="data:audio/wav;base64,${{result.audio}}" type="audio/wav"></audio>`;
                    document.getElementById('chatContainer').appendChild(audioDiv);
                    document.getElementById('chatContainer').scrollTop = document.getElementById('chatContainer').scrollHeight;
                }}
                
                showStatus('Response complete!', 'success');
                
            }} catch (error) {{
                showStatus('Error getting AI response: ' + error.message, 'error');
            }}
        }}
        
        async function testVoice() {{
            const voiceSelect = document.getElementById('voiceSelect');
            const selectedVoice = voiceSelect.value;
            const testText = "Hello! This is a test of the selected voice.";
            
            try {{
                showStatus('Testing voice...');
                
                const response = await fetch('/test-voice', {{
                    method: 'POST',
                    headers: {{
                        'Content-Type': 'application/json',
                    }},
                    body: JSON.stringify({{ 
                        text: testText,
                        voice: selectedVoice
                    }})
                }});
                
                const result = await response.json();
                
                if (result.audio) {{
                    const audio = new Audio(`data:audio/wav;base64,${{result.audio}}`);
                    audio.play();
                    showStatus('Voice test complete!', 'success');
                }} else {{
                    showStatus('Voice test failed', 'error');
                }}
                
            }} catch (error) {{
                showStatus('Error testing voice: ' + error.message, 'error');
            }}
        }}
        
        function clearChat() {{
            document.getElementById('chatContainer').innerHTML = '';
            showStatus('Chat cleared', 'success');
        }}
        
        // Enter key support for text input
        document.getElementById('textInput').addEventListener('keypress', function(e) {{
            if (e.key === 'Enter') {{
                sendText();
            }}
        }});
        
        // Initial message
        addMessage('🤖 Voice Assistant ready! Select a voice and try recording audio or typing messages.');
    </script>
</body>
</html>
    """)

@app.post("/transcribe")
async def transcribe_audio(audio: UploadFile = File(...)):
    """Transcribe uploaded audio to text"""
    try:
        audio_data = await audio.read()
        transcription = assistant.transcribe(audio_data)
        return {"transcription": transcription}
    except Exception as e:
        return {"error": str(e)}

@app.post("/chat")
async def chat(request: dict):
    """Get AI response with thinking and generate speech"""
    try:
        message = request.get("message", "")
        voice = request.get("voice", "lessac_medium")
        
        if not message:
            return {"error": "No message provided"}
        
        # Get AI response with thinking
        thinking, ai_response = assistant.query_ollama_with_thinking(message)
        
        # Generate speech for response only (not thinking)
        audio_base64 = assistant.speak(ai_response, voice)
        
        return {
            "thinking": thinking,
            "response": ai_response,
            "audio": audio_base64
        }
    except Exception as e:
        return {"error": str(e)}

@app.post("/test-voice")
async def test_voice(request: dict):
    """Test a specific voice"""
    try:
        text = request.get("text", "Hello, this is a voice test.")
        voice = request.get("voice", "lessac_medium")
        
        audio_base64 = assistant.speak(text, voice)
        
        return {"audio": audio_base64}
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "message": "Voice Assistant is running"}

if __name__ == "__main__":
    import uvicorn
    print("Starting Voice Assistant Web UI...")
    print("Access at: http://localhost:8765")
    uvicorn.run(app, host="0.0.0.0", port=8765)
